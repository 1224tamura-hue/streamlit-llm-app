import os
import logging
from dataclasses import dataclass
import streamlit as st
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨ï¼ˆCloudã§ã¯ .env ãŒç„¡ãã¦ã‚‚OKï¼‰
load_dotenv()

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# =========================
# å®šæ•°ã¨ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
# =========================
@dataclass
class ExpertConfig:
    """å°‚é–€å®¶ã‚¿ã‚¤ãƒ—ã®è¨­å®š"""
    name: str
    system_message: str


class AppConstants:
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®šæ•°"""
    MODEL_NAME = "gpt-4o-mini"
    TEMPERATURE = 0.3
    PAGE_TITLE = "LangChain LLM App"
    PAGE_ICON = "ğŸ¤–"

    # å°‚é–€å®¶ã‚¿ã‚¤ãƒ—ã®å®šç¾©
    EXPERT_CONFIGS = {
        "Aï¼šã‚­ãƒ£ãƒªã‚¢ç›¸è«‡ã®ãƒ—ãƒ­ï¼ˆè»¢è·ãƒ»è·å‹™çµŒæ­´æ›¸ãƒ»é¢æ¥ï¼‰": ExpertConfig(
            name="Aï¼šã‚­ãƒ£ãƒªã‚¢ç›¸è«‡ã®ãƒ—ãƒ­ï¼ˆè»¢è·ãƒ»è·å‹™çµŒæ­´æ›¸ãƒ»é¢æ¥ï¼‰",
            system_message=(
                "ã‚ãªãŸã¯ã‚­ãƒ£ãƒªã‚¢ç›¸è«‡ã®ãƒ—ãƒ­ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®çŠ¶æ³ã‚’æ•´ç†ã—ã€"
                "ç¾å®Ÿçš„ã§å…·ä½“çš„ãªæ¬¡ã®ä¸€æ‰‹ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚å¿…è¦ãªã‚‰è³ªå•ã‚‚ã—ã¦ãã ã•ã„ã€‚"
            ),
        ),
        "Bï¼šPython/ç”ŸæˆAIã®è¬›å¸«ï¼ˆåˆå¿ƒè€…å‘ã‘ï¼‰": ExpertConfig(
            name="Bï¼šPython/ç”ŸæˆAIã®è¬›å¸«ï¼ˆåˆå¿ƒè€…å‘ã‘ï¼‰",
            system_message=(
                "ã‚ãªãŸã¯Pythonã¨ç”ŸæˆAIã®åˆå¿ƒè€…å‘ã‘è¬›å¸«ã§ã™ã€‚"
                "å°‚é–€ç”¨èªã¯ã‹ã¿ç •ãã€æ‰‹é †ã‚’ç•ªå·ä»˜ãã§å…·ä½“çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
                "å¯èƒ½ãªã‚‰ã‚³ãƒ”ãƒšã§ãã‚‹ä¾‹ã‚‚ç¤ºã—ã¦ãã ã•ã„ã€‚"
            ),
        ),
    }

    DEFAULT_SYSTEM_MESSAGE = "ã‚ãªãŸã¯è¦ªåˆ‡ã§æœ‰èƒ½ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"


# =========================
# ã‚­ãƒ¼å–å¾—ï¼ˆCloudå¯¾å¿œã®è‚ï¼‰
# =========================
def get_api_key() -> str:
    """
    Streamlit Community Cloud: st.secrets ã‹ã‚‰å–å¾—
    ãƒ­ãƒ¼ã‚«ãƒ«: ç’°å¢ƒå¤‰æ•° or .envï¼ˆload_dotenvæ¸ˆã¿ï¼‰ã‹ã‚‰å–å¾—
    """
    api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))
    if not api_key:
        raise ValueError("OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    return api_key


# =========================
# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# =========================
def get_system_message(expert_type: str) -> str:
    """å°‚é–€å®¶ã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—"""
    config = AppConstants.EXPERT_CONFIGS.get(expert_type)
    if config:
        return config.system_message
    logger.warning(f"Unknown expert type: {expert_type}. Using default message.")
    return AppConstants.DEFAULT_SYSTEM_MESSAGE


@st.cache_resource
def get_llm(api_key: str) -> ChatOpenAI:
    """LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦å†åˆ©ç”¨ï¼ˆå®‰å®šï¼†é«˜é€ŸåŒ–ï¼‰"""
    return ChatOpenAI(
        model=AppConstants.MODEL_NAME,
        temperature=AppConstants.TEMPERATURE,
        api_key=api_key,
    )


# =========================
# LLMå‘¼ã³å‡ºã—é–¢æ•°ï¼ˆæ¡ä»¶ï¼šå¼•æ•°2ã¤â†’æˆ»ã‚Šå€¤1ã¤ï¼‰
# =========================
def ask_llm(input_text: str, expert_type: str) -> str:
    """
    å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã¨å°‚é–€å®¶ã‚¿ã‚¤ãƒ—ã‚’å—ã‘å–ã‚Šã€LLMã®å›ç­”ã‚’è¿”ã™
    """
    logger.info(f"LLMå‘¼ã³å‡ºã—é–‹å§‹ - Expert: {expert_type}")

    system_message = get_system_message(expert_type)

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_message),
            ("human", "{input}"),
        ]
    )

    api_key = get_api_key()
    llm = get_llm(api_key)

    chain = prompt | llm | StrOutputParser()
    response = chain.invoke({"input": input_text})

    logger.info("LLMå‘¼ã³å‡ºã—æˆåŠŸ")
    return response


# =========================
# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ç®¡ç†
# =========================
def initialize_session_state():
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "message_count" not in st.session_state:
        st.session_state.message_count = 0


def add_to_history(user_input: str, expert_type: str, response: str):
    st.session_state.chat_history.append(
        {
            "user_input": user_input,
            "expert_type": expert_type,
            "response": response,
            "timestamp": st.session_state.message_count,
        }
    )
    st.session_state.message_count += 1


def display_chat_history():
    if st.session_state.chat_history:
        st.subheader("ğŸ“ ä¼šè©±å±¥æ­´")
        for i, chat in enumerate(reversed(st.session_state.chat_history), 1):
            with st.expander(
                f"ä¼šè©± {len(st.session_state.chat_history) - i + 1}: {chat['expert_type'][:20]}..."
            ):
                st.markdown(f"**è³ªå•:**\n{chat['user_input']}")
                st.markdown(f"**å›ç­”:**\n{chat['response']}")


# =========================
# Streamlit UI
# =========================
st.set_page_config(page_title=AppConstants.PAGE_TITLE, page_icon=AppConstants.PAGE_ICON)
initialize_session_state()

st.title("ğŸ¤– LangChain Ã— Streamlit LLMã‚¢ãƒ—ãƒª")

st.markdown(
    """
### ã“ã®ã‚¢ãƒ—ãƒªã§ã§ãã‚‹ã“ã¨
- å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦é€ä¿¡ã™ã‚‹ã¨ã€**LangChainçµŒç”±ã§LLMã«å•ã„åˆã‚ã›**ã¦å›ç­”ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚
- ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§ã€ŒLLMã«æŒ¯ã‚‹èˆã‚ã›ã‚‹å°‚é–€å®¶ã€ã‚’é¸ã¹ã¾ã™ï¼ˆ**é¸æŠã«å¿œã˜ã¦ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒåˆ‡ã‚Šæ›¿ã‚ã‚Šã¾ã™**ï¼‰ã€‚
- ä¼šè©±å±¥æ­´ãŒä¿å­˜ã•ã‚Œã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ç¢ºèªã§ãã¾ã™ã€‚

### ä½¿ã„æ–¹
1. ã€Œå°‚é–€å®¶ã‚¿ã‚¤ãƒ—ã€ã‚’é¸ã¶ï¼ˆA or Bï¼‰
2. ä¸‹ã®å…¥åŠ›æ¬„ã«è³ªå•ã‚’å…¥åŠ›
3. ã€Œé€ä¿¡ã€ã‚’æŠ¼ã™
"""
)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")

    if st.button("ğŸ—‘ï¸ ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"):
        st.session_state.chat_history = []
        st.session_state.message_count = 0
        st.success("ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
        st.rerun()

    st.metric("ä¼šè©±æ•°", len(st.session_state.chat_history))
    st.divider()
    display_chat_history()


# APIã‚­ãƒ¼å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼ˆCloud/ãƒ­ãƒ¼ã‚«ãƒ«ä¸¡å¯¾å¿œï¼‰
try:
    _ = get_api_key()
except ValueError:
    st.error(
        "OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n\n"
        "- ãƒ­ãƒ¼ã‚«ãƒ«: `.env` ã« `OPENAI_API_KEY=...` ã‚’è¨­å®š\n"
        "- Streamlit Community Cloud: Secrets ã« `OPENAI_API_KEY` ã‚’è¨­å®š"
    )
    st.stop()


expert_type = st.radio(
    "å°‚é–€å®¶ã‚¿ã‚¤ãƒ—ã‚’é¸æŠã—ã¦ãã ã•ã„",
    [
        "Aï¼šã‚­ãƒ£ãƒªã‚¢ç›¸è«‡ã®ãƒ—ãƒ­ï¼ˆè»¢è·ãƒ»è·å‹™çµŒæ­´æ›¸ãƒ»é¢æ¥ï¼‰",
        "Bï¼šPython/ç”ŸæˆAIã®è¬›å¸«ï¼ˆåˆå¿ƒè€…å‘ã‘ï¼‰",
    ],
)

user_text = st.text_area(
    "å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ï¼ˆè³ªå•ãƒ»ä¾é ¼å†…å®¹ï¼‰",
    placeholder="ä¾‹ï¼šè·å‹™çµŒæ­´æ›¸ã®è¦ç´„æ–‡ã‚’æ”¹å–„ã—ãŸã„ / LangChainã®åŸºæœ¬ã‚’æ‰‹é †ã§æ•™ãˆã¦",
    height=140,
    key="user_text",
)

# é€ä¿¡/ã‚¯ãƒªã‚¢
col1, col2, col3 = st.columns([1, 1, 4])
with col1:
    submit_button = st.button("ğŸ“¤ é€ä¿¡", type="primary")
with col2:
    clear_button = st.button("ğŸ”„ ã‚¯ãƒªã‚¢")

if clear_button:
    st.session_state["user_text"] = ""
    st.rerun()

if submit_button:
    if not user_text.strip():
        st.warning("âš ï¸ å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    if len(user_text) > 2000:
        st.warning("âš ï¸ å…¥åŠ›ãŒé•·ã™ãã¾ã™ã€‚2000æ–‡å­—ä»¥å†…ã«ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    with st.spinner("ğŸ¤” LLMã«å•ã„åˆã‚ã›ä¸­..."):
        try:
            answer = ask_llm(user_text, expert_type)
            add_to_history(user_text, expert_type, answer)
            st.success("âœ… å›ç­”ã‚’å–å¾—ã—ã¾ã—ãŸï¼")
        except Exception as e:
            st.error("âŒ LLMå‘¼ã³å‡ºã—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
            logger.exception("Unexpected error during LLM call")
            st.exception(e)
            st.stop()

    st.subheader("ğŸ’¬ å›ç­”")
    st.markdown(answer)

    st.divider()
    col_fb1, col_fb2, col_fb3 = st.columns(3)
    with col_fb1:
        if st.button("ğŸ‘ å½¹ç«‹ã£ãŸ"):
            st.success("ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼")
    with col_fb2:
        if st.button("ğŸ‘ æ”¹å–„ãŒå¿…è¦"):
            st.info("ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚æ”¹å–„ã«åŠªã‚ã¾ã™ï¼")
